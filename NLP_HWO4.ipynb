{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HWO4.ipynb",
      "provenance": [],
      "mount_file_id": "1AQOqbaMq-rGqisCF3PZ1bgwbILXOKhF9",
      "authorship_tag": "ABX9TyNyg6Wdy0OEBLJf+fz5MvNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bresut/NLP-git/blob/master/NLP_HWO4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMp7XtJM2vok",
        "outputId": "6e7e79e9-4270-45f3-bb01-cbac66106d62"
      },
      "source": [
        "pip install opencc-python-reimplemented"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencc-python-reimplemented\n",
            "  Downloading opencc-python-reimplemented-0.1.6.tar.gz (484 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 92 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 484 kB 5.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: opencc-python-reimplemented\n",
            "  Building wheel for opencc-python-reimplemented (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opencc-python-reimplemented: filename=opencc_python_reimplemented-0.1.6-py2.py3-none-any.whl size=486150 sha256=3f56f71d7bc776e8ceb4813c7ab03aa925be205c5175f51e9efcf5183203c3c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/e2/60/d062d260be08788bb389521544a8fc173de9a9a78d6a593344\n",
            "Successfully built opencc-python-reimplemented\n",
            "Installing collected packages: opencc-python-reimplemented\n",
            "Successfully installed opencc-python-reimplemented-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-g2YcU-LY_"
      },
      "source": [
        "import pandas as pd\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from opencc import OpenCC\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CnhBq9y4k4j"
      },
      "source": [
        "data = []\n",
        "for i in range(65,78):#A的asc code為65 M為77\n",
        "    last_num = 100\n",
        "    if i == 77:\n",
        "        last_num = 74\n",
        "    for j in range(last_num):\n",
        "        if j<10:\n",
        "            Path2data = './drive/MyDrive/Colab Notebooks/wiki_zh/A' + chr(i) + '/wiki_0' + str(j)\n",
        "        else:\n",
        "            Path2data = './drive/MyDrive/Colab Notebooks/wiki_zh/A' + chr(i) + '/wiki_' + str(j)\n",
        "            inf = open(Path2data, encoding = 'utf-8')\n",
        "            for line in inf.readlines():\n",
        "                data.append(json.loads(line))\n",
        "                inf.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AmtSX5hOztl"
      },
      "source": [
        "cc = OpenCC('s2t')\n",
        "for element in data:\n",
        "    element['text'] = cc.convert(element['text'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rapXJc5Nyg1v"
      },
      "source": [
        "以下參考http://ilms.ouk.edu.tw/d9534524/doc/43713"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Eg4Dyb9OOm"
      },
      "source": [
        "import jieba"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnvoXwkU91Gt"
      },
      "source": [
        "分詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu01O54bRcMH",
        "outputId": "975868b5-a77f-4af4-feb3-3f59c1cb084a"
      },
      "source": [
        "cut_words = []\n",
        "for i in data:\n",
        "    cut_words.append([' '.join(list(jieba.cut(i['text'],cut_all=False)))])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.065 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9nF0wQ9uWS"
      },
      "source": [
        "with open('output.txt','wb') as f:\n",
        "    for i in range(10):\n",
        "        f.write(cut_words[i][0].encode('utf-8'))\n",
        "        f.write('\\n'.encode('utf-8'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyAxCbeSqQc2",
        "outputId": "3a97a9ec-6056-4fc0-a8bc-6baf1af4f775"
      },
      "source": [
        "pip install word2vec"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 20 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 30 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 530 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.19.5)\n",
            "Building wheels for collected packages: word2vec\n",
            "  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=156423 sha256=36434d3bf7b0bcf56b35597e7fe810e8af29ac55594f52b1af6c01b917ff3ffe\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/c0/d4/29d797817e268124a32b6cf8beb8b8fe87b86f099d5a049e61\n",
            "Successfully built word2vec\n",
            "Installing collected packages: word2vec\n",
            "Successfully installed word2vec-0.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P4tlY022KGT"
      },
      "source": [
        "Word2Vec不能用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "F_QGZfokruVV",
        "outputId": "bbfde6d4-19b7-406f-f99a-830be8fd4f69"
      },
      "source": [
        "myword2vec = Word2Vec('output.txt', size = 250,iter=3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-e78456fe74d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDQtUphGSvL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebe6712-0987-4d83-f05b-2f974422d94b"
      },
      "source": [
        "import word2vec#'corpusWord2Vec.bin有錯誤\n",
        "\n",
        "myword2vec = word2vec.word2vec('output.txt', 'model.txt', size = 300, verbose = True)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running command: word2vec -train output.txt -output model.txt -size 300 -window 5 -sample 1e-3 -hs 0 -negative 5 -threads 12 -iter 5 -min-count 5 -alpha 0.025 -debug 2 -binary 0 -cbow 1\n",
            "Starting training using file output.txt\n",
            "Vocab size: 627\n",
            "Words in train file: 14094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpzyXfR7V-7S"
      },
      "source": [
        "model = word2vec.load('model.txt')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjXpiz2QVary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f59916-0767-4127-d421-ffabc2e30274"
      },
      "source": [
        "import numpy as np\n",
        "word = '巴黎'\n",
        "top20 = 21\n",
        "print(model.vectors)\n",
        "print(model[word])\n",
        "metrics = np.dot(model.vectors, model[word])\n",
        "TOP20_SHOW = np.argsort(metrics)[::-1][1:top20]"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.08014069  0.08849118 -0.07671639 ... -0.02625299 -0.03316167\n",
            "   0.06151729]\n",
            " [ 0.05533562 -0.03616597  0.01241802 ...  0.11488073  0.02238525\n",
            "  -0.05890256]\n",
            " [ 0.05584791 -0.0367599   0.01264409 ...  0.11526303  0.02272362\n",
            "  -0.05832854]\n",
            " ...\n",
            " [ 0.05544383 -0.03764126  0.01356161 ...  0.11511681  0.02384135\n",
            "  -0.0596022 ]\n",
            " [ 0.05751597 -0.03768261  0.01051673 ...  0.11331171  0.02391826\n",
            "  -0.06008997]\n",
            " [ 0.05542651 -0.03823246  0.01491076 ...  0.116576    0.02477054\n",
            "  -0.05832123]]\n",
            "[ 0.05627527 -0.03627376  0.0131184   0.017803   -0.0037794  -0.03188112\n",
            " -0.01959083 -0.07279316 -0.05017395  0.07431922 -0.03217455  0.15585808\n",
            " -0.09233386 -0.06487997 -0.06409524  0.00585567  0.03604775 -0.00293986\n",
            "  0.00151551 -0.06135882 -0.07401612  0.00847484  0.0157745  -0.10598518\n",
            "  0.01519673 -0.07080717 -0.01587358  0.12742775  0.08735261  0.0355632\n",
            " -0.01960197 -0.0293883  -0.01325822 -0.03422562  0.00650409  0.0151982\n",
            "  0.03075402 -0.09423397  0.09716913 -0.04464043 -0.00267046 -0.03697845\n",
            " -0.06789926 -0.02693974  0.02331189 -0.02067279 -0.06965544  0.03155457\n",
            " -0.02655456 -0.06185891  0.07239801  0.0431258   0.03033718 -0.00416018\n",
            "  0.00567628  0.04204677 -0.13417016  0.06780077  0.01765145 -0.0763058\n",
            " -0.05562216  0.07552225 -0.07950801  0.01113006 -0.0458247   0.07833313\n",
            " -0.02878766 -0.03788424 -0.00553176  0.1026449   0.05556881 -0.07992046\n",
            "  0.06858696 -0.12312071 -0.11699564 -0.00787069 -0.05740296 -0.03244189\n",
            " -0.04871853 -0.03765325  0.07065386  0.04178851  0.00381897  0.01189309\n",
            " -0.00111098 -0.01207601 -0.01857541  0.09026316  0.12924167  0.02875864\n",
            " -0.03598179  0.09241536  0.00816294  0.04826417  0.008498    0.04805546\n",
            " -0.00277687  0.05873087  0.06531762 -0.05251141 -0.0298878   0.0424768\n",
            " -0.03155164 -0.06700608  0.06416267  0.01818319 -0.09685607  0.06247274\n",
            "  0.05003119  0.08041879  0.0722321   0.03089941 -0.00793811 -0.03089853\n",
            "  0.0294736   0.07630668 -0.04474244  0.06085698 -0.00869293 -0.00834205\n",
            "  0.09010955  0.02780126 -0.01696404 -0.07653797  0.02217482 -0.04040198\n",
            " -0.07659952 -0.0182084   0.14300526  0.04227189 -0.03225047  0.03978288\n",
            " -0.06404746  0.04992508 -0.09707914  0.02062355  0.03866838  0.02066781\n",
            "  0.00679986  0.01115322 -0.06564974  0.12900834  0.0779245  -0.1106777\n",
            "  0.00589408  0.00122677  0.06873734  0.03658653 -0.12867358 -0.08114518\n",
            " -0.01339219 -0.0013707  -0.03434258  0.03015397  0.00294601  0.00024535\n",
            "  0.03573204 -0.06003766 -0.02295456 -0.12462362  0.07410846 -0.10186253\n",
            "  0.08601269  0.06126795 -0.02086802 -0.05360393 -0.0614793  -0.02591171\n",
            "  0.01570034 -0.04330402  0.07594231  0.09006793  0.01208773 -0.06928931\n",
            " -0.10876469  0.08503362 -0.09320741 -0.02559835 -0.00863431 -0.03553476\n",
            " -0.07069988 -0.13113093 -0.01113504 -0.08641839 -0.01031309 -0.02334971\n",
            " -0.00104708  0.03480075  0.00226095 -0.02075722  0.0403246   0.06980347\n",
            "  0.03860536 -0.02966472 -0.02984178  0.07093468  0.02888557 -0.00553264\n",
            " -0.03567898  0.00445302  0.01230876 -0.04022757  0.04405386 -0.05140512\n",
            "  0.01511143 -0.03970168  0.06018042  0.00411767  0.03836967 -0.00307411\n",
            " -0.10352343 -0.02474239 -0.0788209   0.01062411  0.05261166  0.05457215\n",
            "  0.07183343  0.06703188  0.05338144  0.04616356  0.03710216  0.04136171\n",
            "  0.01945423 -0.01189339  0.00497597 -0.02668647 -0.00088175 -0.0458288\n",
            "  0.01558865  0.01228795 -0.05127555  0.00581258 -0.1045966   0.0765922\n",
            "  0.06957805  0.05270664  0.05853564 -0.04605129 -0.06628438  0.00380431\n",
            "  0.03088153  0.02586452 -0.0384752   0.07427203  0.02271214  0.04824922\n",
            "  0.00674153 -0.04752342 -0.02823334 -0.02512288 -0.0005039  -0.05066172\n",
            "  0.13487574  0.01061531 -0.10581399 -0.07025988  0.09864419  0.01635051\n",
            " -0.00138565 -0.03834564  0.07258767 -0.02203558 -0.12344668 -0.04720067\n",
            " -0.05544306  0.06667425  0.08033085  0.01637836  0.06098009  0.02590907\n",
            " -0.12347218  0.0143012   0.0157487   0.06840287 -0.01718331 -0.05664697\n",
            " -0.02137397  0.06558555 -0.06760906  0.07676368 -0.0436895   0.03203267\n",
            " -0.02463716 -0.02488925 -0.05748006  0.14763358  0.00406843 -0.02876392\n",
            "  0.07088602  0.02564496  0.02424348 -0.05985504  0.01379349  0.07267854\n",
            "  0.07601062  0.04990807 -0.02229676  0.11484432  0.02262771 -0.05955164]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkkv1K0hVwef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7b4539-145d-4f24-d250-077b84431802"
      },
      "source": [
        "count = 1\n",
        "for top in TOP20_SHOW:\n",
        "  print(\"第{num}名:{word}\".format(num=count, word=model.vocab[top]))\n",
        "  count=count+1"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "第1名:年\n",
            "第2名:後\n",
            "第3名:。\n",
            "第4名:世紀\n",
            "第5名:和\n",
            "第6名:於\n",
            "第7名:開始\n",
            "第8名:中心\n",
            "第9名:而\n",
            "第10名:城市\n",
            "第11名:有\n",
            "第12名:由\n",
            "第13名:也\n",
            "第14名:許多\n",
            "第15名:至\n",
            "第16名:巴黎市\n",
            "第17名:都\n",
            "第18名:歐洲\n",
            "第19名:世界\n",
            "第20名:法國\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}