{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HWO4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AQOqbaMq-rGqisCF3PZ1bgwbILXOKhF9",
      "authorship_tag": "ABX9TyN90lhHWEKpm8IQ2Kfupnp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bresut/NLP-git/blob/master/NLP_HWO4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMp7XtJM2vok",
        "outputId": "43bb4c7a-06e4-461d-af3e-157cb79b5085"
      },
      "source": [
        "pip install opencc-python-reimplemented"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencc-python-reimplemented\n",
            "  Downloading opencc-python-reimplemented-0.1.6.tar.gz (484 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 61 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 92 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 122 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 133 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 174 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 184 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 194 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 204 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 215 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 225 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 235 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 245 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 256 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 266 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 276 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 296 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 307 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 327 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 337 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 348 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 358 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 368 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 378 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 389 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 399 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 409 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 419 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 430 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 440 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 450 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 460 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 471 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 481 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 484 kB 10.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: opencc-python-reimplemented\n",
            "  Building wheel for opencc-python-reimplemented (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opencc-python-reimplemented: filename=opencc_python_reimplemented-0.1.6-py2.py3-none-any.whl size=486150 sha256=6430d07875f9bddac0b728eaaa1c7a6f87131a43edf3d004410f756dd89dc00a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/e2/60/d062d260be08788bb389521544a8fc173de9a9a78d6a593344\n",
            "Successfully built opencc-python-reimplemented\n",
            "Installing collected packages: opencc-python-reimplemented\n",
            "Successfully installed opencc-python-reimplemented-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-g2YcU-LY_"
      },
      "source": [
        "import pandas as pd\n",
        "from opencc import OpenCC\n",
        "import json\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CnhBq9y4k4j"
      },
      "source": [
        "data = []\n",
        "for i in range(65,78):#A的asc code為65 M為77\n",
        "    last_num = 100\n",
        "    if i == 77:\n",
        "        last_num = 74\n",
        "    for j in range(last_num):\n",
        "        if j<10:\n",
        "            Path2data = './drive/MyDrive/Colab Notebooks/wiki_zh/A' + chr(i) + '/wiki_0' + str(j)\n",
        "        else:\n",
        "            Path2data = './drive/MyDrive/Colab Notebooks/wiki_zh/A' + chr(i) + '/wiki_' + str(j)\n",
        "            inf = open(Path2data, encoding = 'utf-8')\n",
        "            for line in inf.readlines():\n",
        "                data.append(json.loads(line))\n",
        "                inf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AmtSX5hOztl"
      },
      "source": [
        "cc = OpenCC('s2t')\n",
        "for element in data:\n",
        "    element['text'] = cc.convert(element['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rapXJc5Nyg1v"
      },
      "source": [
        "以下參考http://ilms.ouk.edu.tw/d9534524/doc/43713"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Eg4Dyb9OOm"
      },
      "source": [
        "import jieba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnvoXwkU91Gt"
      },
      "source": [
        "分詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu01O54bRcMH",
        "outputId": "48910f8d-544e-46a6-b6a8-803af36a917e"
      },
      "source": [
        "cut_words = []\n",
        "for i in data:\n",
        "    cut_words.append([' '.join(list(jieba.cut(i['text'],cut_all=False)))])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.021 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9nF0wQ9uWS"
      },
      "source": [
        "with open('output.txt','wb') as f:\n",
        "    for i in range(10):\n",
        "        f.write(cut_words[i][0].encode('utf-8'))\n",
        "        f.write('\\n'.encode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyAxCbeSqQc2",
        "outputId": "02857407-1bae-49a3-e4ba-b2267e6472d8"
      },
      "source": [
        "pip install word2vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting word2vec\n",
            "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 20 kB 28.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 30 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 40 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 927 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.0.1)\n",
            "Building wheels for collected packages: word2vec\n",
            "  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=156423 sha256=da59e178f018b14526deb5aff2bf06e6374b217612f0ca46c5e21160fbf98f21\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/c0/d4/29d797817e268124a32b6cf8beb8b8fe87b86f099d5a049e61\n",
            "Successfully built word2vec\n",
            "Installing collected packages: word2vec\n",
            "Successfully installed word2vec-0.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P4tlY022KGT"
      },
      "source": [
        "Word2Vec不能用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "F_QGZfokruVV",
        "outputId": "bbfde6d4-19b7-406f-f99a-830be8fd4f69"
      },
      "source": [
        "myword2vec = Word2Vec('output.txt', size = 250,iter=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-e78456fe74d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDQtUphGSvL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ae576b-21e9-40d4-aeba-d5b61cf6272b"
      },
      "source": [
        "import word2vec#'corpusWord2Vec.bin有錯誤\n",
        "\n",
        "myword2vec = word2vec.word2vec('output.txt', 'model.bin', size = 300, verbose = True,binary=True)\n",
        "print('done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running command: word2vec -train output.txt -output model.bin -size 300 -window 5 -sample 1e-3 -hs 0 -negative 5 -threads 12 -iter 5 -min-count 5 -alpha 0.025 -debug 2 -binary 1 -cbow 1\n",
            "Starting training using file output.txt\n",
            "Vocab size: 627\n",
            "Words in train file: 14094\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sitq0aE-RoT"
      },
      "source": [
        "def loadModel(binFielPath):\n",
        "  return word2vec.load(binFielPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjXpiz2QVary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2626e07a-0aee-409a-f4eb-a646fb4f25c3"
      },
      "source": [
        "import numpy as np\n",
        "import codecs\n",
        "search_word = '巴黎'\n",
        "model = loadModel(\"/content/model.bin\")\n",
        "top20 = 20\n",
        "datas = model.similar(search_word,n=top20)\n",
        "word = list(datas[0])\n",
        "score = list(datas[1])\n",
        "for i in range(len(word)):\n",
        "  print(\"%s : %s\"%(model.vocab[word[i]],score[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "後 : 0.999967913488281\n",
            "年 : 0.9999663219664491\n",
            "。 : 0.9999655327983548\n",
            "於 : 0.9999632912514308\n",
            "和 : 0.99996264796382\n",
            "開始 : 0.9999619017599191\n",
            "世紀 : 0.9999618969836575\n",
            "而 : 0.9999610089563767\n",
            "有 : 0.9999608062790013\n",
            "法國 : 0.9999603423285726\n",
            "中心 : 0.999959808599831\n",
            "許多 : 0.999958800941255\n",
            "由 : 0.9999587041788971\n",
            "也 : 0.9999585392831605\n",
            "世界 : 0.9999577000210085\n",
            "都 : 0.9999563492169031\n",
            "巴黎市 : 0.9999560096852356\n",
            "爲 : 0.9999559231296073\n",
            "就 : 0.9999558925751794\n",
            ", : 0.999955519453152\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}